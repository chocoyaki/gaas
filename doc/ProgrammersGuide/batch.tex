ServiceTable prend serivice mais pas mise à jour. A voir. On pourrait
virer des services d'un serveur qui ne répond pas ou plus.

-------------------------------------------------------

This chapter contains things that must be dispatched along the
programmer's guide (and even more detailed?). It is actually in
writing, and contains, for the most, remarks.\\

The concern of this chapter is batch submission and how we achieve to
do it in \textsc{Diet}. In consequence, it deals with all the
structures that are used in the recording of problems and the
submission of jobs. Mechanisms are explained. Tools on which we rely,
like Elagi and Appleseeds are also mentionned.

Note that in the following, a {\it task} refers to the {\it resolution
of a problem} which has been {\it requested} by the client.

\section{Notations in this chapter in pseudo code parts}

``.fonction()'' is written to know rapidly it is a call. If a
tabulation appears after, then the following describes the called
fonction. Should be replaced by UML graphs.

\section{Notes on structures}

\subsection{Generalities}
\begin{itemize}
\item A changement in a structure which has to be transfered : CORBA
Must look in idl, etc.

\item Conversion between structures is done in CORBA/marchalling
\end{itemize}

\subsection{About batch}

\paragraph{Configuration and compilation.}$ $\\
Brief explanations about stuff in Makefile and in configure.in...\\

Note that to compile \textsc{Diet} with the batch availability, one
must use the \verb$--enable-Batch$ , \verb$--with-Elagi=$ and
\verb$--with-Appleseeds=$ options.

\paragraph{Implementation.}$ $\\
We must put a \textsc{Diet} reqID in the profile because we only know
the batch ID when submitting the script which is built when calling
the function given in the programmer SeD. We need to establish a
correspondance between both IDs, done in the SeD.

\subsection{Main structures: sum-up}
Here, we present the structures used along the recording of problems
and the submission and resolution of tasks. 

Note that the \verb$corba_pb_desc_t$ can be accessed through the
\verb$corba_request_t$ structure.

\newpage

\pagestyle{empty}

\begin{minipage}[h]{.3\linewidth}
\begin{verbatim}
typedef struct {
  char*       pb_name;
  int         last_in, last_inout, last_out;
  diet_arg_t* parameters;

  const void* SeDPtr; /* pointer to SeD object, to be used in
                      ** performance estimation
                      ** And for batch submission */
#ifdef HAVE_BATCH
  unsigned short int batch_flag ;
  int nbprocs ;
  unsigned long walltime ;
  // Used for correspondance batch job ID / DIET job ID
  int dietJobID ;
#endif

} diet_profile_t;
\end{verbatim}
\begin{verbatim}
/** Service profile descriptor (mapping for diet_profile_desc_t) */
struct corba_profile_desc_t {
  string path;
  long   last_in;
  long   last_inout;
  long   last_out;
  sequence<corba_arg_desc_t> param_desc;
#if HAVE_BATCH
  long batch_flag ;
#endif
  corba_aggregator_desc_t aggregator;
};
\end{verbatim}
\begin{verbatim}
/**
 * Actually, this is an equivalent to a diet_profile_t without the data.
 */
struct corba_pb_desc_t {
  string path;
  long   last_in;
  long   last_inout;
  long   last_out;
  SeqCorbaDataDesc_t param_desc;
#if HAVE_BATCH
  long   batch_flag ;
  long   nbprocs ;
  long   walltime ;
#endif
};

/** The complete problem, sent from client to server. */
struct corba_profile_t {
  long   last_in;
  long   last_inout;
  long   last_out;
  SeqCorbaData_t parameters;
#if HAVE_BATCH
  long   batch_flag ;
  long   nbprocs ;
  long   walltime ;

  unsigned long dietJobID ;
#endif
};
\end{verbatim}
\end{minipage}

\newpage

\pagestyle{plain}

\begin{figure}
\begin{center}
\includegraphics[width=14cm]{./fig/DiagrammeGridRPCDietStructure.eps}
\caption{Structures used between GridRPC parts}
\end{center}
\end{figure}

\section{Notes on recording of services}

\subsection{Generalities}

Numerous services are statically declared in a SeD. If a server must
provide a problem resolution, either another SeD which can perform it
is launched on the the server or the corresponding SeD is stopped and
another one, whose code has been improved and compiled for the
architecture, is launched.

The code of a SeD can be found in \textsc{Diet}\_server and SeDImpl
files. The first one, written in C, launches a deamon, whose code is
in the second. If you want to add thing to the API, ask yourself if
your changes must consider dynamic stuff (like take into account
CORBA) or is completely static. For example, queues and batch
submission requires dynamic information and therefore are implemented
in SeDImpl.cc.

To add a service, the SeD calls \verb$add_table_service$ and
comparisons are made in order to know if the service is already
declared. Each field is tested!

. addService()
. Each SeD give to the localAgent the list of problems it can solve.
. LocalAgents give their lists to the agent immediately superior in the hierarchy, etc., until the agent is a MA.

$\rightarrow$ This is done to know if it is necessary to transmit a
request on a branch of the hierarchy.

Figure representing each part of the platform and kind of structure
that is used (\verb$diet_profile_desc_t$ in SeD, then corba.. in
communication, etc.)

\subsection{Recording of a batch service}

Addition and management of services : \verb$corba_profile_desc_t$. This 
structures contains the \verb$batch_flag$.

.addService() No batch and normal submission allowed.
Explanation of the test...

\paragraph{How it works.}$ $\\
In \verb SeDImpl::run() where we obtain the batch scheduler name. At
beginning, the SeD reads the config file to know if it is a
batch/parallel SeD or not by searching for the .  BATCHNAME is defined
as a field in a enum structure in \verb$src/utils/Parsers.hh$. The
corresponding string is defined in \verb$src/utils/Parsers.cc$. One
can read in this file that the string that the SeD is searching in its
config file is \verb$batchName$. Correct values are: "shell",
"condor", "dqs", "loadleveler", "lsf", "pbs", "sge", "oar".

\section{Request submission}
\subsection{Synchronous}
\verb$diet_call$, \verb$diet_call_batch$ (no in official API)

\begin{verbatim}
.diet_call_common()
  .request_submission()
    diet_profile_t in corba_pb_desc_t
    data management
    .MA->submit(corba_pb_desc_t, )
    determine chosenServer, dietJobID
  send Datas
  .chosenServer->solve(char* jobName, corba_profile_t, reqID)
  get Datas
\end{verbatim}

\subsection{Asynchronous}
\verb$diet_call_async$

\begin{verbatim}
.diet_call_async_common(diet_profile_t, SeD_var& chosenServer, reqID)
\end{verbatim}

\subsection{Notes on jobs submission}

\paragraph{Paragraph 1.}$ $\\
3 kind of submissions : batch, parallel and sequential

\begin{itemize}
\item sequential is already ok ;)

\item batch and parallel are both traited the same way: elagi use shell or
batch in a perl script to launch the job (which is a script containing
the mpirun command with all good options and the executable name of
the parallel job to launch)
\end{itemize}

\paragraph{Paragraph 2.}$ $\\
It is possible to have queues AND batch submission in a SeD

\paragraph{Paragraph 3.}$ $\\
When calling \verb$diet_submit_batch()$, the SeD programmer must provide the
desired way of submission among:

\begin{itemize}
\item \verb$DIET_Lam$,
\item \verb$DIET_Mpich$,
\item \verb$DIET_Pvm$,
\item \verb$DIET_Sequential$.
\end{itemize}

That way, the SeD Programmer can specified which MPI implementation to
use. Of course, one should be sure that it has in its \$\verb$PATH$ and
in its \$\verb$LD_LIBRAY_PATH$ the {\bf right path to the
implementation} he wants to use, as well as the executable compiled
and ready to use where the SeD is deployed.

\section{General Remarks}

\subsection{Howto refer to batch reserved nodes?}
You can use the macro \verb$BATCH_NBNODES$ and \verb$BATCH_NODESFILE$
in the command elaborated in the SeD. Diet will replace them by the
corresponding batch macro where the job is executed.

The SeD programmer does not have to precise MPI commands and options
(typically,
\verb$mpirun -np BATCH_NODESFILE$). This is done by \textsc{Diet}. Indeed,
MPI submission may differ from an implementation to another, and the
running Sed does not know which one is used onto the cluster. Thus,
the access has to be transparent.

\subsection{Batch Scheduler: generalities}
Elagi provides a way to submit jobs onto cluster via batch
schedulers. Originally, it is meant to be used from a station apart
from the cluster. The send is done remotely to the frontal where the
elgi script is executed.

But we can use Elagi to perform the script which is forked on the
frontal. This is what is done in \textsc{Diet}.

Elagi can be used to submit jobs on the following batch schedulers:
"shell","condor","dqs","loadleveler","lsf","pbs","sge","oar".
% FIXME: do we give shell, condor which are not batch?
These are the names that must be incorporated in server.config to let
the SeD know how to submit a job correctly. In Elagi, batch schedulers
are of  type
\verb$ELBASE_SchedulerServiceTypes$. Any discusion with Elagi must be preceded by a call to \verb$ELBASE_GiveBatchID()$ which return the Elagi batch ID.

\subsection{Loadleveler}
The environment variable \$\verb$LOADL_PROCESSOR_LIST$ gives all hostnames
for the current job. Hostnames are not unique (you can ask for several
jobs per host) and the variable is not set if the number of hosts is
greater than 128!  You must {\huge specify} in performance predictions
that if the batch is Loadleveler (accessible with
\verb$(SeDImpl*)profile->SeDPtr)->getLocalHostName()$), the number of
requested hosts can not be greater than 128.

Elagi always put the sequence \verb$"#@ job_type=parallel\n"$ in the script.

\section{How to run}

Do not forget to specify in the SED.cfg the 


\section{Notes diverses and TODO}

\begin{itemize}
\item Add the necessary when solve is called to stock the address of
the SeDImpl in order to access some information about the SeD from the
resolution (which consist in batch mode to submit the job the the
batch scheduler and manage data and executionof the job). This is not
done in the normal solve !
\verb$ profile.SeDPtr = (const void*) this ; $

\item To do: explain the test in DIET\_data.cc::profile\_match() (why do
it only if batch\_flag==1). The reason commes from
ServiceTable::lookupService(), which is called from
MasterAgent::submitLocal(), AgentImpl::findServer(),
SeDImpl::getRequest(), SeDImpl::checkContract(),
DIET\_server.diet\_service\_table\_lookup\_by\_profile.

Thus, the check must be performed in special way concerning batch
\begin{itemize}
\item if batch is asked, strict check
\item if nothing specified, both batch and non-batch must be considered
\end{itemize}

\item Add in src/utils/Parsers.cc the information to parse batchName
in config file.

\item Added in src/CORBA/idl/common\_types.idl, the unsigned long
dietJobID. Be sure that it is well managed (see
mrsh\_profile\_to\_in\_args and unmrsh\_in\_args\_to\_profile). The dietJobID
is stocked in the profile (src/client/DIET\_client.cc).

\item Look if src/CORBA/idl/response.idl batch\_flag is correctly used
and where. Same with common\_types.idl

\end{itemize}
