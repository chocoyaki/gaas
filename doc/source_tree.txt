// textsf ---> <TT></TT>
/**
\page DoxySourceTree You know this and that...

There are two main source directories, <TT>include</TT> and <TT>src</TT>,
and the \c doc directory.


\section SectionDoc The doc subdirectory

This directory contains documentations that are to be maintained up-to-date,
  even in CVS, (<EM>ie</EM> <B>not only for releases ! </B>):

<UL>  
<LI> The <EM>User's Manual</EM>, in <TT>UsersManual</TT>. Its output file is
  <TT>DIET_UsersManual.ps</TT> which is intalled in
  <TT>{$<$install_dir$>$/doc</TT>
</LI>
<LI> This <EM>Programmer's Guide</EM> in <TT>ProgrammersGuide</TT>. Its output
  file is <TT>DIET_ProgrammersGuide.ps</TT>, which is intalled in
  <TT>$<$install_dir$>$/doc</TT>, <B>if</B> DIET has been configured in
  maintainer mode.
</LI>
<LI> A tutorial, but only in the CVS version, since the distribution of this
  tutorial is apart from the distribution of the source code.
</LI>
</UL>

  The compilation of this documentation may require too recent versions of 
  Doxygen and LaTeX and of various converters for the figures. That is why,
  the output <TT>.ps</TT> files are included in the distributions.


  The makeam of the distributed documents have this particular that they build
  the names of the initial <TT>.tex</TT> to compile and the 
  <TT>.ps</TT> file to generate from the name of the directory (stored in the
  variable <TT>++). %$</TT>

  Finally, to avoid having several copies of the same figure, the
  <EM>Programmer's Guide</EM> uses the DIET logo of the <EM>User's Manual</EM>,
  which complicates a bit the <TT>Makefile.am</TT>.


\section SectionInclude The include subdirectory

  This directory contains all the files necessary for the DIET user to program his
  application. Almost all these files are headers which are copied into the
  <TT>$install_dir$/include</TT> directory, because they describe the
  complete API of DIET, for the client side as well as for the server side.

  Nevertheless, there is also a <TT>Makefile.inc.in</TT> which will be
  configured into a <TT>Makefile.inc</TT>. The latter is to be included by the
  <TT>Makefile</TT> of the DIET user, to set all options and variables
  necessary for compiling and linking with the DIET libraries.
  It may be noticed that some variables are affected with
  <TT>?=</TT> it is because the <TT>Makefile.inc</TT> is also included by the
  DIET examples, which need to define a different prefix as long as the
  <TT>make install</TT> has not been invoked.


\subsection SubSectionDIET_data DIET_data.h

  The first file to be included. It describes all types and data structures
  (and the associated functions and macros) that are common to the interfaces
  of the client and the server:
  <TT>diet_arg_t+</TT>,
  <TT>diet_profile_t</TT>,
  <TT>*_set</TT>
  and
  <TT>*_get</TT> functions,
  <TT>diet_free_data</TT>, etc.

  Please note that the <TT>*_get</TT> functions are actually macros, which
  allows the user not to care about the type of the data.
  But the compilation of a program that uses such macros could trigger a
  warning about ``strict aliasing''.
  This is the case for the distributed examples, when compiled with
  <TT>gcc-3.3</TT> or later (see section \ref SectionSrcExamples ).


\subsection  SubSectionDIET_client DIET_client.h and DIET_grpc.h

  The specific part of the client interface is described in these two files.
  <TT>DIET_client.h</TT> includes <TT>DIET_grpc.h</TT>, which includes itself
  <TT>DIET_data.h</TT>.
  The first file is the DIET specific API, adpated and optimized for the data
  structures of DIET. The second file consists of the GridRPC API, even if
  some of the functions are not fully implemented (for more details, please
  read the numerous comments of the file itself).

\subsection SubSectionDIET_server DIET_server.h

  This file contains the specific part of the server API.
  It offers functions and macros to manipulate the description of the service
  profiles (<TT>diet_profile_desc_t</TT> and store them into the service table
  of the server (<TT>diet_*service_table*</TT>).
  Everything that concerns the problem profiles (how to extract their
  arguments, set their OUT arguments, etc.) is described in
  <TT>DIET_data.h</TT>.


  There is also the complete API to the <TT>diet_convertor_t</TT> structures.
  Their usage are (or should be) explained in the <EM>User's Manual</EM>,
  and the way they are applied is fully documented in section
  \ref SectionSrcCORBA , since the conversions are performed by functions of
  the file <TT>marshalling.cc</TT>.

\subsection SubSectionDIET_mutex DIET_mutex.h

  This is intented to be a portable API to the various thread libraries, a bit
  like the <TT>omnithread</TT> library.
  But the <TT>omnithread</TT> library is a portable API for all thread
  systems (pthreads for Linux, but also the official thread libraries of
  other platforms), and <TT>DIET_mutex.h</TT> should only unify the APIs of
  different ORBs, since they all offer a portable thread library.
  Thus, as well as an application using <TT>omniORB</TT> should use the
  <TT>omnithread</TT> library if it has to deal with threads, an application
  using DIET will have to use the DIET thread library.

  It is not documented yet in the <EM>User's Manual</EM> because it is really
  not ready to be distributed.
  Currently it deals only with mutexes, and waits for a complete development...


\section SectionSrcAgent src\/agent

  \subsection SubSectionHierarchyOfAgent The hierarchy of agents

  This section deals with the inheritance of the C++ agent classes and the
  way an agent refers to its children.
  The inheritance hierarchy of the CORBA objects will be treated in
  section \ref SectionSrcCORBAIdl .

  The hierarchy of the C++ classes that implement the CORBA interfaces is
  mapped onto the hierarchy of theses interfaces, <EM>i.e.</EM>
  <TT>LocalAgentImpl</TT> and
  <TT>MasterAgentImpl</TT> both inherit from <TT>AgentImpl</TT>. Thus, most
  of the public function members of each class are implementations of
  IDL methods.

  In <TT>AgentImpl</TT>, <TT>getName</TT> is especially added for the data
  persistency architecture.

  In all classes, it has been added a <TT>run</TT> method that makes the
  <TT>main</TT> function (in <TT>dietAgent.cc</TT>) launch the agent.
  This method has two parts: the generic part is declared and implemented in
  the <TT>AgentImpl</TT> class, and the parts specific to the LA or the MA are
  declared in these classes.
  As these declarations overwrite the <TT>run</TT> method of
  <TT>AgentImpl</TT>, their implementations call explicitly the parent method.

  The protected part of the <TT>AgentImpl</TT> is inherited by both the
  <TT>LocalAgentImpl</TT> and <TT>MasterAgentImpl</TT>, and it groups the
  common members (the <TT>ServiceTable</TT>, the lists of children and
  requests, etc.) as well as the common parts of the code: the way to
  forward a request, to aggregate the responses, and some methods useful
  in all these jobs.

  The architecture of agents is handled with a pointer to the parent
  (<TT>Agent_var parent;</TT>) in <TT>LocalAgentImpl</TT>, and two vectors
  of children in <TT>AgentImpl</TT>, one for LAs and one for SeDs.
  These vectors of children are actually <TT>ts_vectors</TT> of instances
  of <TT>NodeDescription</TT>.
  The latter has to store information such as the hostname, the child ID
  (since children are referred to with their indexes in the vectors) but
  above all the CORBA reference of the child (an <TT>LA_var</TT> or an
  <TT>SeD_var</TT>).
  That is why it has to be a template.


  \subsection SubSectionRequests The requests

  The requests are managed through a class that stores all informations
  about the requests, such as their ID, the responses returned by the SeD and LA
  and the scheduler to use for the request.

  A mechanism have been developed to manage the waiting of all the responses
  returned by the SeD and LA.
  The <TT>waitResponses()</TT> methods will block the current thread until
  a defined number of call of the <TT>addResponse()</TT> method have
  been made.
  Look at the doxygen documentation for more information.


  \subsection SubSectionSchedulers The schedulers

  When all the children of an agent have sent back their responses, the
  agent must aggregate and sort them into the one single response it will
  send to its parent.
  This aggregation/sort is performed by the scheduler classes.

  The starting point of the development of these scheduler classes was a
  combination of two ideas:
<UL>
<LI>
  Some servers would never have FAST fully installed. Either because FAST or
  NWS does not support the hardware architecture, or because the services
  offered cannot be described in the FAST ``language'' accurately enough for a
  valid bench campaign or estimation, etc. The information that the servers can
  give to the scheduler are very heterogeneous, and the old DIET scheduler was
  dealing with only the FAST estimations.
  An other tool has been implemented, called CoRI, and will be used to get by 
  default the information and to get other types of information (total
  memory,...).
  At the moment it is not compiled\/called by default. This will be change
  when it is matured.
  See section \ref SubSectionPerformance for more information.
</LI>
<LI>
  For research aspects on the platform, it would be very interesting to
  have pluggable schedulers. As the old scheduler was integrated to the agent,
  the first step towards pluggable schedulers was building classes responsible
  for the scheduling.
</LI>
<LI> \todo{plugin is made by alan su!}
</LI>
</UL>

  The heterogeneity of the information given by the servers is due to the fact
  that all servers cannot provide the same level of accuracy about the
  evaluation of a request.
  We have thought of 4 different levels of accuracy, and one
  <TT>Scheduler</TT> class deals with one and only one level.
  <UL>
  <LI>
    No information at all: such servers are not sorted but placed randomly.
  </LI>
  <LI>
    Last solve time: such server are sorted according to the last solve time.
  </LI>
  <LI>
    Hardware information (disk space, RAM, CPU speed, etc.): not managed yet.
  </LI>
  <LI>
    Dynamic information (CPU load, free memory, given by NWS): such servers
    are sorted according to their weight, which is a polynom of these
    characteristics, including the data transfer time if available.
  </LI>
  <LI>
    Estimation of the computation time (performed by FAST): such servers
    are sorted according to the computation time estimated plus the data
    transfer time estimated.
  </LI>
  </UL>

  Actually, there are only four schedulers implemented, because nothing has been
  done yet for the servers to get static information (level 3).\\
  Actually, with the compiling option --enable-cori only scheduling with round
  robin (last solve time - level 2) and random scheduling (level 1) is possible.
  There are multiple reasons why no other scheduling is possible:
  <UL>
  <LI>
    The NWS scheduling is bogus: the weight for the memory is too
    heavy in comparison with the free CPU. Fixing is not so easy, so
    we deciding to test with several plugin schedulers their
    performance, before implementing a default scheduling.
  </LI>
  <LI>
     To avoid an increasing complexity of the context.
  </LI>
  <LI>
    To avoid a single server equipped with FAST will always be chosen by
    the agent.
  </LI>
  <LI>
    The plugin scheduler can be used now with all power, it will
    be possible to define a scheduler for each server.
    It is very easy to recreate the old fast scheduling by the plug-in.
  </LI>
  </UL>

  Once the schedulers have sorted every group of servers, the most difficult is
  still to do. How to combine the various information of the servers to get a
  whole sorted list ?
  Which parameters should we give priority to ?
  This is the role of the <TT>GlobalScheduler</TT> classes.
  They are schedulers of the schedulers.
  A <TT>GlobalScheduler</TT> has got a sorted list of schedulers
  (<TT>SchedList schedulers</TT>) to apply onto the lists of servers. This
  presents the drawback that the agent sorts by category of information before
  sorting the servers inner each category: it is not possible to mix the various
  categories yet. But it would be easy to implement schedulers that take into
  account all type of information, computing a weight as for NWS information.
  It is just not done, because our goal was to build a frame for developping
  new schedulers, and not to test various policies.

  The only <TT>GlobalScheduler</TT> available so far (<TT>StdGS</TT>) calls
  first the <TT>FASTScheduler</TT> onto the list, putting ahead the servers
  that could estimate the request, then the <TT>NWSScheduler</TT>
  (for NWS information), then the <TT>RRScheduler</TT> and finally the
  <TT>RandScheduler</TT> that add the servers randomly into the list.
  \\

  Several drawbacks can be noticed about such a structure of schedulers:
  <UL>
  <LI>
    It is not possible to mix the various categories of servers.
  </LI>
  <LI>
    The schedulers are not pluggable at all.
  </LI>
  <LI>
    A global scheduler must be associated to each request, so that every
    agent, at every level, will schedule it the same way. But if we let the
    servers decide, they will choose different schedulers ; so only the MA can
    choose it, and as soon as the request arrives from the client ! Then it
    forwards the scheduler (its serialzed form actually) with the request,
    before it gets any information about the servers.
  </LI>
  <LI>
    The classes <TT>GlobalScheduler</TT> and <TT>Scheduler</TT> are actually
    virtual (because of their respective <TT>sort</TT> method). Thus, they
    constitue an interface that every new scheduler (global or not) should
    implements, and some maintainers may find this interface a bit too rigid,
    but it is the only way I found to get the scheduler out of the agent.
  </LI>
  </UL>

  Despite these drawbacks, it is now much easier to change the scheduler used
  in the agent.
  But there is still a lot to do to reach the pluggable schedulers, if
  ever it is possible.

\subsection SubSectionPerformance Performance prediction
  CoRI is now implemented to get more information about the SeD.
  It is called by the server's side and (in FAST mode) on the agent's side. 
  CoRI is a simple super layer that manages multiple sub modules. One module
  is Cori-Easy; another is FAST.

  \subsubsection SubSubSectionUsingCori Using Cori
  The programmer of DIET services will use the
  interface proposed in the SeD interface file, <TT>DIET_server.h</TT>
  Using CoRI as programmer DIET, use directly the
  <TT>CORIMgr.hh</TT>
  The starting phase of cori can be described as follow:
  <UL>
  <LI>
    All desired collectors will be add by <TT>add</TT>
  </LI>
  <LI>
    Launch the <TT>startCollectors</TT> to initialising some collectors
    (i.e FAST) on startup of the server
  </LI>
  <LI>
     using the <TT>call_cori_mgr</TT> function on 
     demand to get a value in the estVector.
  </LI>
  </UL>
  
  Additional to the tags that the programmer of DIET
  service can use, the DIET programmer can use 
  the tag EST_COMMTIME for FAST. It is agent side only. 

  \subsubsection SubSubSectionCoRIEasy CoRI-Easy
  Cori-Easy is based on basic system calls. There are different
  levels to obtain this information: 
  The best way is the most portable way: The standard C library.
  For examples the functions <TT>get_Write_Speed</TT> and
  <TT>get_Read_Speed</TT> are written in pure C.
  OmniORB is used by DIET and it needs the GNU Library C standard, 
  so this library is the next level.
  The third level is the POSIX standard. Remark: even if Red Hat 
  say they follow the POSIX standard, this doesn't mean that you have
  to include the same file to get the function of the POSIX standard
  (for example <TT>getPID()</TT>). 
  It means you can never be sure to have the function!
  The fourth level is the distribution of the operating system, 
  but even here you have to take care(new version-old version conflicts). 
  System calls are very fragile, point of view accessibility 
  (they are not always present or you have to be root to execute them),
  point of view functionality of the function (``top -n 1'' 
  in Linux is different to ``top -n 1'' in Mac).
  and point of view presentation of the output data (is it sure that 
  he will print it always on the second line?).

  Very important to avoid problems in the compilation is the use of 
   <TT>{\#ifdef HAVE_nameFunction</TT>.  
  The calls to cori-easy are in reality a set of tests. If the first function
  doesn't work, the next one is tested.
  In this way we avoid to write for each distribution a test to identify
  this distribution and we avoid to have to find out the right function for
  this special distribution.

  \todo
  <UL>
  <LI> 
    Problems with  the function <TT>diet_est_cori</TT> called twice in the
    same tag for a scalar values, the second will erase the first
  </LI>
  <LI>
     including of the class diagram: source file can be modified by
     MS visio source file available on
     <TT>/home/CVS/tarballs/sourcefile_coriArchitect.tar</TT>
     on graal 
     \image html fig/FAST.png "CoRI architecture"
     \image latex fig/FAST.eps "CoRI architecture" width = 15cm
  </LI>
  </UL>

\subsubsection SubSubSectionAvailability Availability
  CoRI Easy will be used to initialize the vector by default.
  The problem is that even Cori-Easy is not always able to call a function
  that gives the correct value.
  That's why it is important to deliver the default value.
  \todo Set up a table with the default values

\subsubsection  SubSubSectionExtensibility Extensibility
  CoRI is supposed to be extensible, by adding a new API and a new tag
  of type <TT>diet_est_collect_tag_t</TT> in the CoRI class structure.
  The file <TT>Cori_Data_Easy</TT> is used as interface between
  CoRI and the Cori-Easy collector.
  The same architecture is used for FAST: the file
  <TT>FASTMgr</TT> is used to interface between Cori_Fast and the library of
  FAST.

\subsubsection SubSubSectionExamplesScheduler Examples with the scheduler
  See <TT>/home/CVS/tarballs/scheduler_cori_examples.tar.gz</TT>
  on the graal server (perhaps bogus).

\subsubsection SubSubSectionFastCollectorCoRI FAST as collector of CoRI
  Fast will be launched on the server side only on demand of the
  plugin scheduler.
  FAST will be launched automatically on the MA and LA side.

  \todo{launching only if need?}

  Would it be interesting to be able to start fast on the MA side
  dynamicaly?
  \todo {FAST without CoRI}\\ 
  In this case all possible fields of the vector will be filled (i.e.
  erase old values).

\section SectionSrcClient src/client

  This directory contains the implementation of both client APIs, the DIET
  API and the GridRPC API.

  The GridRPC is actually incompatible with the strictly defined types used in
  DIET, which avoid many run-time errors.
  That is why the <TT>gridrpc_call</TT> functions is a bit modified, as
  it requires <TT>diet_arg_t</TT> arguments only.

  \todo{Christophe - the asynchronous part of the API}


\subsection SubSectionSynchronousCall Synchronous call in DIET
  Synchronous DIET call are based on <EM>twoway</EM> CORBA call.
  CORBA manages synchronizing and memory.
  Main problems and architectures related to this kind of call concerns
  DIET types, modified memory management increasing performance and
  convertors (see section \ref SectionSrcCORBA ).

  \subsection SubSectionAsynchronousCall Asynchronous call in DIET

  \subsubsection SubSubSectionArchitecture Architecture

  <EM>omniORB</EM> is chosen because of its performance.
  But its implementation of standard CORBA specification is not full.
  For instance, <EM>AMI</EM> (Asynchronous Message Interface) or sending
  object by value are not yet available.
  According to this lack of services, we must based asynchronous
  communication architecture on its avalaible services.
  Two basic CORBA mecanisms can allow this asynchronous architecture.
  First is using IDL key word <EM>oneway</EM> which delegates
  <EM>thread</EM> managing to <EM>POA</EM> (Portable Object Adaptor) and
  stops immediatly after sending data parameters.
  This thread executes solver code and return explicitly results data to client.
  Secondth uses standard CORBA synchronous call.
  <EM>RPC</EM> creates a thread, delegates algorythm executing to it and
  returns.
  Executed algorythm solves its problem and return results data to clients
  or stores it (waitting for client get it).

  Getting a result from an asynchronous call can be done by <EM>polling</EM>
  or <EM>callback</EM> mecanism.

  <UL>
  <LI>
    <EM>Callback mecanism</EM> allows SeD process to bind client and send
    results data.
    During asynchronous call (solveAsync methode)to begin SeD solve, client
    add to send data its CORBA reference identifiing a local CORBA server
    waiting results and a request identifier.
    As soon as results are available, they are sent to client callback
    CORBA server with its request identifier by calling <EM>notifyRst</EM>
    server's function.
    Data structures sent to SeD server or return to client callback server
    are like them of synchronous call.
    Currently, client callback server reference and request identifier
    are volatile.
  </LI>
  <LI>
    <EM>Polling mecanism</EM> will use next DIET data persistency mecanism.
    Results data will be store locally or exported to a persistency service.
    They will be avalaible through DIET persistency API.
    This mecanism is under development as polling service.
    It will replace callback when presistence will by critical.
  </LI>
  <LI>
    [DIET release 1.0] provides only asynchronous with callback function.
    Its implementation is hidden by DIET and GridRPC API on client side.
    All asynchronous call and synchronized waiting results functions
    are <EM>thread-safe</EM>.
  </LI>
  </UL>

\subsubsection SubSubSectionCoding Coding
  \paragraph ParagraphCallback (Callback)
  Callback 's module is part of DIET client library.
  It contains classes generated from <EM>callback.idl</EM> file by CORBA
  idl compiler and CallbackImpl class which provides functions for waiting
  and managing results.
  It is a <EM>singleton</EM> <EM>thread-safe</EM> class.

  \image html  fig/CorbaClientClassDIagram.png "UML Class diagram about Callback mecanism"
  \image latex fig/CorbaClientClassDIagram.ps "UML Class diagram about Callback mecanism" width=15cm
  \anchor FigureCorbaClientClassDIagram

  Class constructor is private. It is accessed by a static class function
  <EM>CallbackImpl::instance</EM> which returns a sole object instance.
  It creates object, initialises its class data and CORBA context, provides a
  client callback CORBA server on network.
  Client code must only give its CORBA reference when calling asynchronous
  function <EM>solveAsync</EM>.
  SeD server calls <EM>CORBA</EM> <EM>solveResult</EM> function with this
  reference to send results to client callback server.
  Combining  a call to <EM>diet_initialise</EM> and DIET file option
  <EM>useAsyncCall</EM> automates client callback mecanism.
  A <EM>diet_finalize</EM> call unactivates callback server, destroys
  CallbackImpl object and Object Request Broker data.

  \paragraph ParagraphSynchronizingData Synchronizing data
  Synchronising access on data is necessary because of:
  <UL>
  <LI>
     Threads can together retrieve results data.
  </LI>
  <LI>
    CORBA threads pool from client callback server read concurrently
    profil data structure.
  </LI>
  <LI>
    DIET API function <EM>diet_wait*</EM> stop caller thread until wait
    results data are arrived on client.
  </LI>
  </UL>
  Classes from following UML diagram resume these drawbacks.

  \image html  fig/CallBackSynchronisationClassDiagram2.png "Diagramme UML class diagram about callbackImpl and GridRPC/DIET asynchronous functions"
  \image latex fig/CallBackSynchronisationClassDiagram2.ps "Diagramme UML class diagram about callbackImpl and GridRPC/DIET asynchronous functions"
  \anchor FigureCallBackSynchronisationClassDiagram2

  CallAsyncMgr manages synchronising mecanism on wait results.
  It is a <EM>thread-safe</EM> singleton which locks access on share data
  and race condition code.
  A <EM>diet_initialise</EM> / <EM>{diet_finalise</EM> call creates/destroys
  it.
  A Reader/Writer lock mecanism (exception-safe) manages syncronising between
  CORBA and DIET client threads.

  \image html  fig/WaitRulesClassDiagram.png "FIXME: NO CAPTION YET"
  \image latex fig/WaitRulesClassDiagram.ps  "FIXME: NO CAPTION YET"

  <EM>CallAsyncMgr</EM> register/unregister asynchronous calls, their request
  identifiers and data by <EM>addAsyncCall</EM> / <EM>deleteAsynCall</EM>
  methods.
  It stops client threads with addWait* methods.
  These methodes have parameters which store conditions on retrieved
  results to unlock threads.
  A <EM>Rule</EM> structure contains a set of three data (request identifier,
  logical operator, state).
  Two list of data pair (<EM>RulesConditionMap</EM> and <EM>CallAsyncList</EM>)
  map <EM>Rule</EM> with a lock reference and request identifier with
  a DIET data profil.
  <EM>notifyRst</EM> methode allows CORBA threads to announce results getting,
  update data in memory client space, unlock one or more waited threads.

  \paragraph ParagraphSynchronisingInterworking Synchronising and interworking
  In following scenarios, an UML actor figures user client code with
  DIET\/GridRPC API.

  \paragraph ParagraphAsynchronousCall Asynchronous call on a SeD:
  This first scenario shows a begining of an asynchronous solve.

  \image html  fig/CallAsyncSequenceDiagram.png "FIXME: NO CAPTION YET"
  \image latex fig/CallAsyncSequenceDiagram.ps  "FIXME: NO CAPTION YET"

  After creating CallbackImpl and callAsyncMgrby a call to
  <EM>diet_initialize</EM>, client execute submit function specifying DIET
  problem.
  Then, <EM>addcallAsync</EM> methode stores a sole request
  identifier and managed memory space and <EM>solveAsync</EM> starts
  asynchronous solve.
  CallAsyncList is a C++ container <EM>list</EM> from <EM>Standard Template
  Library</EM>.
  <EM>ORB</EM> entity is a virtual object figuring CORBA services API like
  marshalling, network traffic and CORBA object binding.
  Client thread can wait for notify event about satisfied conditions or
  poll results states by frequently call to <EM>getStatusReqId</EM> methode.


  \paragraph ParagraphThreadWaiting Thread Waiting for results and then continue:
  Solve is started and client thread can wait for results.

  \image html  fig/CallAsyncWaitSequenceDiagram.png "FIXME: NO CAPTION YET"
  \image latex fig/CallAsyncWaitSequenceDiagram.ps  "FIXME: NO CAPTION YET"

  Client code can use synchronising functions like <EM>diet_wait*</EM>
  in GridRPC/DIET API to wait for results. The following text describes
  this mecanism.

  After calculus starting, client creates a waiting Rule which specifies
  waiting conditions. Then it calls <EM>addWaitRule</EM> methode which do :

  <UL>
  <LI>
    locks synvhronising mecanism.
  </LI>
  <LI>
    validates rule data (request identifier, etc).
  </LI>
  <LI>
    registers the rule.
  </LI>
  <LI>
    creates an <EM>omniORB</EM>  semaphore, maps to the rule, registers it,
    unlocks synchonising mecanism (<EM>WriterLock</EM>) and call its
    <EM>wait</EM> method.
  </LI>
  <LI>
    during call to <EM>notifyRst</EM> method by <EM>Callback</EM> server
    (<EM>calbackImpl</EM>), finds all rules whose conditions are verified
    and unlock their semaphores.
  </LI>
  <LI>
    frees memory linked to rule data and returns a <EM>STATE</EM> enum
    which defines five state (RESOLVING, WAITING, DONE, CANCEL, ERROR).
    If a calculus is stopped during its executing process (with a call to
    deleteCallAsync for example), CANCEL is returned.
    If thread catches an error or an exception, ERROR is returned else
    DONE is returned.
  </LI>
  </UL>


  \paragraph ParagraphCallbackResults Getting results into callback server and notifying them to waiting threads :
  This scenario figures notifying available results into client. It completes
  the previous algorythm about threads using client gridRPC API which is a
  part of DIET client library.

  \image html  fig/CallbackSynchronisationSequenceDiagram.png "FIXME: CAPTION"
  \image latex fig/CallbackSynchronisationSequenceDiagram.ps  "FIXME: CAPTION"

  <UL>
  <LI>
    SeD pushes results on client by a CORBA call to <EM>solveResults</EM>.
  </LI>
  <LI>
    Callback server notifies available results by a call to <EM>notifyRst</EM>.
  </LI>
  <LI>
    locks synchronising mecanism (<EM>WriterLock</EM>).
  </LI>
  <LI>
    validates data and request identifier(<EM>find</EM>).
  </LI>
  <LI>
    finds rules linked to retrieved results.
  </LI>
  <LI>
    verifies all rules waiting conditions.
  </LI>
  <LI>
    unlocks waiting threads linked to these rules.
  </LI>
  <LI>
    unlocks synchronising mecanism (<EM>~WriterLock</EM>).
  </LI>
  </UL>

  \paragraph ParagraphGettingAsynchronous Getting result state by an asynchronous function:

  Unlike local synchronous call as <EM>addWait*</EM>, client can get results
  state with <EM>getStatusReqId</EM>.
  This call returns an enum type <EM>STATE</EM> which defines five state
  (RESOLVING, WAITING, DONE, CANCEL, ERROR).
  <EM>CallAsyncMgr::addWait*</EM> returns also this enum.
  RESOLVING announces that calculus goes on, WAITING that is not started,
  DONE that is finished and results are availabale, CANCEL that is stopped
  by user and ERROR that is stopped by an error or an exception.

  \image html  fig/CallAsyncProbeSequenceDiagram1.png "FIXME: CAPTION"
  \image latex fig/CallAsyncProbeSequenceDiagram1.ps  "FIXME: CAPTION"

  <UL>
  <LI>
    call of <EM>CallAsyncMgr::getStatusReqId</EM>.
  </LI>
  <LI>
    lock synchronising mecanism.
  </LI>
  <LI>
    verify calculus identity and validity.
  </LI>
  <LI>
    get state linked to calculus.
  </LI>
  <LI>
    unlock synchronising mecanism.
  </LI>
  </UL>

  <B>NB</B> This algorythm is also used by <EM>diet_probe</EM>
     function into GridRPC API.

  \paragraph ParagraphMemory Managing memory in asynchronous request:

  <EM>CallAsyncMgr::deleteAsyncCall</EM> frees internal data linked to a
  calculus if it is finished, its results are available and no rule includes
  it into its waiting conditions.
  If others rules are linked to this result, their state are modified to
  CANCEL as well as calculus state linked. Then synchronising mecanisms
  are unlocks, threads goes on and <EM>CallAsyncMgr::addWait*</EM> returns
  CANCEL state.
  Finally, memory is free.

  \image html  fig/DietCancelSequenceDiagram.png "FIXME: CAPTION"
  \image latex fig/DietCancelSequenceDiagram.ps  "FIXME: CAPTION"

  <UL>
  <LI>
    call of <EM>callAsyncMgr::deletecallAsync</EM>.
  </LI>
  <LI>
    lock synchronising mecanism.
  </LI>
  <LI>
    verify request identifier.
  </LI>
  <LI>
    look for rules linked to request identifier,
  unlock synchronising items and fix rule state with CANCEl.
  </LI>
  <LI>
    free memory alocated to rule data and mutexes.
  </LI>
  <LI>
    free data about request (request identifier, state, ...)
  </LI>
  </UL>

  <B>NB</B> These algorythms only concern client processus.
    Next development steps will provide direct interactions to SeD server
    processus using persistence service.

\section SectionSrcCORBA src/CORBA

  \subsection SubSectionORBMgr ORBMgr

  This module has been written to hide the ORB to the rest of the code.
  The idea is that every ORB-dependent operation might be performed through
  a unified API, described in <TT>ORBMgr.hh</TT>.
  Of course, DIET still supports only omniORB, but soon it should support
  TAO too.
    
  This module does not aim at unifying the APIs of the various thread
  libraries of such ORBs.
  As this should be a part of the DIET API, it should be processed in
  what <TT>DIET_mutex</TT> could become.
  
  It has been decided to add to this module some of the usual OMG calls, so that
  it is easier to initialize, bind its name in the naming service, etc.


  \subsection SubSectionDataMemoryManagement Data memory management

  \todo Bruno - Help me please, if I have no time ...


  \subsection SubSectionConvertors Convertors

  \todo Under construction ...



  \section SectionSrcCORBAIdl src/CORBA/idl

  In this directory, all IDL interfaces have been gathered. They are
  organized as follows:
  <UL>
  <LI>
    <TT>common_types.idl</TT>: all (almost all) types used in all the
    communications of the platform.
  </LI>
  <LI>
    <TT>response.idl</TT>: the structure of a response, with the sorted
    (scheduled) lists of servers.
  </LI>
  <LI>
    <TT>[Local|Master]Agent.idl</TT>, <TT>Callback.idl</TT> and
    <TT>SeD.idl</TT>: the interfaces of the CORBA objects of the hierarchy.
  </LI>
  </UL>

  Note that the type <TT>corba_estimation_t</TT> should be declared in
  <TT>response.idl</TT>, since it is a part of a response.
  But this would introduce a crossed-dependency: <TT>response.idl</TT>
  includes <TT>SeD.idl</TT> because a response contains the references to
  the servers, and <TT>SeD.idl</TT> needs the <TT>corba_estimation_t</TT>
  to be declared for the method <TT>checkContract</TT>.
  So, to get rid of the crossed-dependency, the type is declared in
  <TT>common_types.idl</TT>.

  Compiling the IDL interfaces is a bit more complex than for any C or C++ file.
  Automake has no mechanism to manage IDL files, except the
  <TT>BUILT_SOURCES</TT> variable.
  This variable stores all the C++ files generated by the compilation of the
  IDL files
  <B>NB</B>:
  The names of these files depend from the IDL compiler, but most of the
  compilers let the user choose the nomenclature.

  But we must still specify the dependencies of the built sources from the
  IDL files.
  This is the role of the file <EM>idlRules.mk</EM>.

  <TT>idlRules.mk</TT> is included in Makefile.am defining compiling and
  linking rules with idl generated stubs files (with omniORB, xxSK.cc and
  xxDynSK.cc).
  It contains a list of idl files, rules, targets, list of cleaned files
  and fixes a python bug.

  \todo{ Christophe - Explain the <TT>idlRules.mk</TT> - its role in the
        compilation chain}

\section SectionSrcExamples src/examples

  This directory contains the examples distributed with DIET. Its compilation
  depends on the configure option <TT>--enable-examples</TT>, forced in
  maintainer, and disabled by default for users.

  There are five complete applications, providing a server and one or several
  clients able to use the service(s) offered by the server. Two of them require
  special libraries to be installed on the machine, which are more or less
  detected at configuration time, but there is still much work to do for
  the automatic detection.

\subsection SubSectionFileTransfer file_transfer

  This example was aimed to show how to manipulate the <TT>DIET_FILE</TT> type.
  The client sends two files to the server. The second file is re-transfered
  without any change, since it is an INOUT, and its size is returned as an OUT.
  It shows the drawback of the file management: it is impossible for the user to
  gain control over the path of the received files.

  \subsection SubSectionScalars scalars

  This example was aimed to show how to manipulate the <TT>DIET_SCALAR</TT>
  type, and the various modes of passing arguments.
  It shows the problems of the management of the base types in DIET: the
  length of the given data is not checked, and the correspondance with the
  CORBA types is still to be perfected.
  Actually, the length of the base types has been defined according to the
  NetSolve types, but it would have been more clever to accord them with
  the CORBA types.

  \subsection SubSectionDmatManips dmat_manips

  The server offers an interface to some basic double matrix manipulations:
  transpose, product and sum.
  To make the clients of the examples <TT>dmat_manips</TT>, <TT>BLAS</TT>
  and <TT>ScaLAPACK</TT> interchangeable for some problems, the profiles
  for services <TT>MatPROD</TT>, <TT>SqMatSUM</TT> (square matrix sum)
  and <TT>SqMatSUM_opt</TT> (``optimized'' because the second operand
  is INOUT) are the same.
  Thus, for a demo, a matrix product can be invoked by a client, and executed
  either on a server having the BLAS library installed, or a server performing
  only a basic matrix product (usually far much slower), according to the
  FAST estimation.

  Although there are many examples of convertors in this and other examples,
  the service <TT>T</TT>, the first written, was designed to give an example
  of remanipulation of the arguments.
  When FAST 0.4 has been integrated into DIET, all other services had to
  include convertors.

  Another step has been done for FAST 0.8.
  The convertors had to be rewritten since the matrix type has been
  suppressed from the FAST API (with all high-level descriptions of the
  problems).
  The idea is to create a profile, through the convertor, that has got only
  the arguments declared in the LDAP base (which can be extracted from the
  others), followed by the arguments that the computation needs.
  It was not possible to do that with FAST 0.4 because it was not designed
  to ignore unknown arguments. That is why we are forced to include
  <TT>DIET_config.h</TT>, to get the version of FAST.

  \todo{Christophe - Please explain the asynchronous clients.}

  <TT>dmat_manips</TT> contains also client samples about asynchronous call API.
  All use <TT>diet_call_async</TT> to start solve on SeD but each example
  shows a distinct function of asynchronous DIET API to get results.
  Like synchronous client, you can ask solving a SUM/PROD/T operation on SeD.
  Besides, they are <TT>multithreaded</TT>. You can specify thread number of
  a pool and solve number.
  Each thread cucurently sends data to SeD(s) and gets results according to
  its used <TT>diet_wait/probe_*</TT> function.
  <TT>parallelclient.cc</TT> retrieves results by a simple <TT>diet_wait</TT>
  function and <TT>parallelclient2.cc</TT> by a peridical <TT>diet_probe</TT>
  call.
  <TT>parallelclient3.cc</TT> and <TT>parallelclient3.cc</TT> are examples
  about respectivly <TT>diet_wait_and</TT> and <TT>diet_wait_or</TT>.
  Executing one of these examples prints traces about calculus parameters,
  synchronizing mecanisms on threads and results.

\subsubsection SubSubSectionBLAS BLAS

  To compile the BLAS server, you will need the BLAS library installed. For
  developers using a Linux Debian, there is a package, and everything has been
  performed for the configure to take such a package into account: just use the
  <TT>--enable-BLAS</TT> option, and it should be automagically detected.
  For other installations, it might be necessary to use all the arguments
  defined by the ACI_PACKAGE macros (<TT>--with-BLAS=</TT>, ...)

  So far, this server provides the <TT>dgemm</TT> service and all its
  derivations:
  <TT>MatPROD</TT>, <TT>SqMatSUM</TT>, <TT>SqMatSUM_opt</TT> and
  <TT>MatScalMult</TT>.
  Convertors are used to convert the profiles of the problems to the one
  of the pdgemm service, so that only one solve function has to be
  implemented.
  But this server is destined to be an interface to all the functions of
  the BLAS library.

  We have the same problem of compatibility with FAST 0.4 and FAST 0.8 in
  the BLAS server as in the dmat_manips example.
  Actually, this server only supports FAST 0.4, since the modifications to
  get low-level profiles for FAST 0.8 have not been done yet.
  But these modifications are quite similar to the ones of the
  dmat_manips server, and they will be easy to add.

  \subsubsection SubSubSectionScaLAPACK ScaLAPACK

  It is very difficult to detect all the libraries necessary to compile a
  program calling the <TT>pdgemm</TT> function.
  Nothing has been done actually for it in the <TT>configure.ac</TT>,
  because not only the LAPACK library should be found, but also an MPI
  implementation, etc.
  Nevertheless it is still possible to compile the server of this example
  if the user knows exactly all the paths for these libraries, or the extra
  arguments to compile with them (through <TT>--with-ScaLAPACK-extra</TT>),
  thanks to the <TT>ACI_PACKAGE</TT> macros from M. Quinson.

  So far, this server provides the <TT>pdgemm</TT> service and all its
  derivations: <TT>dgemm</TT>, <TT>MatPROD</TT>, <TT>SqMatSUM</TT>,
  <TT>SqMatSUM_opt</TT> and <TT>MatScalMult</TT>.
  Convertors are used to convert the profiles of the problems to the one
  of the pdgemm service, so that only one solve function has to be implemented.
  But this server is destined to be an interface to all the functions of
  the ScaLAPACK library.

  The implementation of this SeD is complex. It is launched on the node 0 of an
  MPI grid. When the client submits a problem, it asks for a number of nodes to
  execute the computation. Then, the node 0 creates a subgrid of the available
  nodes and launches the computation on this new grid.
  Of course, depending on the number of nodes required by the client, and
  the number of nodes still available in the initial grid, it is possible
  that the computation returns with an error code, or that the same node
  is used for several processes.

  The support for FAST is quite difficult to add, since the server is
  supposed to be parallel.
  Maybe we should wait for the FAST/Freddy integration to be complete.


  \section SectionSrcSeD src/SeD

  This directory contains the implementations of
  <UL>
  <LI>
    the DIET server API,
  </LI>
  <LI>
    the IDL interface of the SeD.
  </LI>
  </UL>

  The functions of the API are easy to understand, as far as the User's
  Manual is well understood.
  They use a global static variable <TT>SRVT</TT>, and its manipulation is
  not thread-safe, since it would have no sense to launch two SeDs
  with the same service table in two different threads.

  There is nothing else important to explain in the implementation of the IDL
  interface but the <TT>solve</TT> methods.
  These methods run on the same scheme, except that <TT>solveAsync</TT>
  is <EM>oneway</EM>, ie. asynchronous, and thus it has to send explicitly
  the results, through a callback mechanism, at the end of the computation.

  \todo{Christophe - the callback mechanism might require some more
        explanations}
  In case of asynchronous call, i.e solve_async call, client specifies a
  request identifier and a CORBA reference defining its Callback client
  server (IP/Port).
  And thus, at the end of the solve, the <TT>solveAsync</TT> function call
  <TT>notifyRst</TT> of client callback server with diet profile and
  request ID parameters.
  Client manages callback threads, results and notifying news through its
  asynchronous API.
  After this calling, SeD thread free memory.

  Both methods have to ``unmarshall'' the arguments, ie. to convert them into
  DIET data that the programmer of the server can manipulate.
  Then they invoke the solve function through the pointer given by this
  programmer, and reconvert (marshall) the OUT data into CORBA data to be
  sent back on the client.
  Thus the computation is performed inner a CORBA thread, created at the
  remote invokation of the method.

  To perform the computation in a separate process, we will be forced to
  dump the arguments into files ... It is still to be done.


  \section SectionSrcUtils src/utils

  This directory gathers all tools and utils that are used by more than one
  entity of DIET.

  \todo{under construction ...}

  \subsection SubSectionCounter Counter

  The <TT>Counter</TT> class implements a thread safe counter.
  This counter accepts only 32 bits positive value.
  This class must be used each time a counter is shared by several threads.
  Read the doxygen documentation for more informations about how-to use it.

  \subsection SubSectionMsFunction ms_function

  Several calls to <TT>strdup()</TT>, <TT>CORBA::string_dup()</TT>,
  <TT>malloc()</TT> and <TT>stralloc()</TT> are made inside the DIET code.
  The problem is : each of these functions can return the <TT>NULL</TT> value.
  The use of an allocated memory without testing the <TT>NULL</TT> value will
  probably result to a segmentation fault if there was not enough memory.
  This is why the <TT>ms_function</TT> file describes some functions which
  allocate the memory and test if the <TT>NULL</TT> value is returned.
  If it is the case, a <TT>bad_alloc</TT> exception is thrown, if not the
  functions behave like the standard ones.
  Read the doxygen documentation for more informations about how-to use them.

  \subsection SubSectionTsContainer ts_container

  The C++ Standard Library gives access to several containers.
  A map, a vector, a list and a set templates are defined by the C++ Standard.
  The only problem with them is that there are not thread safe.
  If two threads alter a container at the same time, a crash is on the way.
  Therefore a critical section access mechanism must be implemented to
  avoid it.
  It is easy to forget to lock the access to a list before using it.
  This is why the <TT>ts_container</TT> exists (the <TT>ts</TT> is for
  Thread Safe).
  They managed the access to there critical section by themselves.
  All the list and set which are shared by several threads must be
  implemented with a <TT>ts_container</TT> to avoid a simultaneous
  access to it.
  Read carefully the doxygen documentation to know which one to use
  and how-to use it.

*/
